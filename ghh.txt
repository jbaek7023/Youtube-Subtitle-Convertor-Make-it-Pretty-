 here's a visualization of the error function we're standing on top for mounting Mount Everest and want to descend but it's not that easy because it's cloudy and the mountain is very big so we can't really see the big picture what we'll do to go down is we'll look around us and we consider all the possible directions in which we can walk then we pick a direction that makes us descend the most let's say it's this one over here so we take a step in that direction thus we've decreased the height once we take the step then we start the process again and again always decreasing the height until we go all the way down the mountain minimizing the height in this case the key metric that we use to solve the problem is the height will call the height the error the error is what's telling us how badly we're doing at the moment and how far we are from an ideal solution and if it constantly takes steps to decrease the error then we'll eventually solve our problem descending from Mount errors some of you may be thinking wait that doesn't necessarily solve the problem what if I get stuck in a valley a local minimum but that's not the bottom of the mountain this happens a lot in machine learning and we'll see different ways to solve this later in the nanodegree it's also worth noting that many times a local minimum will give us a pretty good solution to a problem this method which was studying more data later it's called gradient descent so let's try that approach to solve a problem what would be a good error function here what would be a good way to tell the computer how badly it's doing well here's our line with our positive and negative area and the question is how do we tell the computer how far it is from a perfect solution well maybe we can count the number mistakes there are two mistakes here so that's our height that's our error so just as we did to descend from the mountain we look around all the directions in which we can move the line in order to decrease our error so let's say we move in the direction will decrease the number of errors to one and then if we move it in the direction will decrease the number of errors to zero and then we're done right well almost there's a small problem with that approach in our algorithms we'll be taking very small steps and the reason for this is calculus because our tiny steps will be calculated by derivative so what happens if we take very small steps here we start with two errors and then move a tiny amount and we're still at two errors then move a tiny amount again and we're still at two errors another tiny amount and we're still at two and again and again so not much we can do here this is the equivalent to using gradient descent to try to descend from an Aztec pyramid with flat steps if we're standing here in the second floor for two errors and we look around ourselves we'll always see two errors and we get confused and not know what to do on the other hand and mount errors we can detect very small variations in height and we can figure out in what direction it can decrease the most in math terms this means that in order for us to the gradient descent our error function cannot be discrete it should be continuous mount error is discontinuous in small variations in our positions will translate to small variations in the height but the Aztec pyramid is not since the height jumps from two to one and then from one to zero as a matter of fact our error function needs to be differentiable but we'll see that later so what we need to do here is to construct an error function that is continuous and we'll do this as follows so here are six points with four of them correctly classified that's two blue and two red and two of them incorrectly classified that is this red point at the very left and this blue point at the very right the error function is going to assign a large penalty to the two incorrectly classified points and small penalties to the four correctly classified points here we are representing the size of the point as the penalty the penalty is roughly the distance from the boundary when the point is misclassified and almost zero when the point is correctly classified we learn the formula for the error later in the class so now we obtain the total error by adding all the errors from the corresponding points here we have a large numbers it to misclassified points at a large amount of the error and the idea now is to move the line around in order to decrease this error but now we can do it because we can make very tiny changes to the parameters of the line which will amount to very tiny changes in the air so if we move the line say in this direction we can see that some errors decrease some slightly increase but in general when we consider the sum the sum gets smaller and we can see that because we've now correctly classified the two points that remains classified before so once we are able to build an error function with this property we can now use gradient descent to solve our problem so here's a full picture here we are the summit of Mount Everest we're quite high up because our error is large as you can see the error is the height which is the sum of the blue and red areas we explore around to see what direction brings us down the most or equivalently what direction can we move the line to reduce the error of the most and we take a step in that direction so in the mountain we go down one step and in the graph we've reduced the error a bit by correctly classifying one of the points and now we do it again we calculate the error we look around ourselves to seeing what direction we descend the most we take a step in that direction and that brings us down the mountain so on the Left we have reduced the height and successfully descended from the mountain and on the right we have reduced the error to its minimum possible value and successfully classified our points now the question is how do we define this error function that's what we will do next
