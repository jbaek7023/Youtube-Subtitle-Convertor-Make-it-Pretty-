 so now we're finally ready to get our hands into training a neural network so let's quickly recall feed-forward we have our perception with a point coming in labeled positive on our equation W 1 X 1 plus W 2 X 2 plus B where W 1 and W what the perceptron does is it plots a point and returns the probability that the point is blue which in this case is small since the point is in the red area thus this is a bad perceptron since it predicts that the point is red when the point is really blue and now let's recall within the gradient descent algorithm we did this thing called back propagation we went in the opposite direction we asked the point what do you want the model to do for you and the point says well I'm misclassified so I want this boundary to come closer to me and we saw that the line got closer to it by updating the weights namely in this case let's say that it tells the weight w1 to go lower and the weight w2 to go higher and this is just an illustration it's not meant to be exact so we obtained new weights W 1 Prime and W 2 prime which define a new line which is now closer to the point so what we're doing is like descending from Mount error is right the height is going to be the error function of W and we calculate the gradient of the error function which is exactly like asking the point what does it want the model to do and as we take the step down the direction of the negative of the gradient we decrease the error to come down the mountain this gives us a new error U of W Prime and then you model W prime with a smaller error which means we get a new line closer to the point we continue doing this process in order to minimize the error so that was for a single perception now what do we do for multi-layer perceptrons well we still do the same process of reducing the error by descending from the mounting except now since the error function is more complicated than it's not Mount errors now it's mount kilimanjaro but same thing we calculate the error function and its gradient we then walk in the direction of the negative of the gradient in order to find a new model W Prime with a smaller error e of W prime which will give us a better prediction and we continue doing this process in order to minimize the error so let's look again at what feed-forward does in a multi-layer perceptron the point comes in with coordinates X 1 and X 2 and label y equals 1 it gets plotted in the linear models corresponding to the hidden layer and then as this layer gets combined the point gets plotted in the resulting nonlinear model in the output layer and the probability that the point is blue is obtained by the position of this point in the final model now pay close attention because this is the key for training neural networks it's back propagation will do as before we'll check the error so this model is not good because it predicts that the point will be red when your reality the point is blue so we'll ask the point what do you want this model to do in order for you to be better classified and the point says I kind of want this blue region to come closer to me now what does it mean for the region to come closer to it well let's look at the two linear models in the hidden layer which one of these two models is doing better well it seems like the top one is battling is classifying the point whereas the bottom one is classifying it correctly so we kind of want to listen to the bottom one more and to the top one less so what we want to do is to reduce the weight coming from the top model and increase the weight coming from the bottom model so now our final model will look a lot more like the bottom model than like the top model but we can do even more we can actually go to the linear models and ask the point what can these models do to classify you better and the point will say well the top model is misclassifying me so I kind of want this line to move closer to me and the second model is correctly classifying me so I want this line to move farther away from me and so this change in the model will actually update the weights let's say it'll increase this to and decrease these two so now after we have paid all the weights we have better predictions at all the models in the hidden layer and also a better prediction at the model in the output layer notice that in this video we intentionally left the bias unit away for clarity in reality when you update the weights we're also updating the bias unit if you're the kind of person who likes formality don't worry we'll calculate these gradients in detail soo