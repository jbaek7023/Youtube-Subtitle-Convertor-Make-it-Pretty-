 let's look a bit closer into cross-entropy why switching to a different example let's say we have three doors I know this is not the Monty Hall problem we have the green door the red door and the blue door and behind each door we could have a gift or not have a gift and the probabilities of their being a gift behind each door is 0.8 for the first one 0.7 for the second one and point one for the third one so for example behind the green door there's an 80% probability of there being a gift and a 20% probability of there not being a gift so we can put the information in this table where the probabilities of there being a gift are given in the top row and the probabilities of there not being a gift are given in the bottom row so let's say we want to make a bet on the outcome so we want to try to figure out what is the most likely scenario here and for that we'll assume they're independent events in this case the most likely scenario is just obtained by picking the largest probability in each column so for the first store it's more likely to have a gift and not have a gift so we'll say there's a gift behind the first door for the second door it's also more likely that there's a gift so we'll say there's a gift behind the second door and for the third door it's much more likely that there's no gift so we'll say there's no gift behind the third door and as the events are independent the probability for this whole arrangement is the product of the three probabilities which is 0.8 times point 7 times 0.9 which ends up being point 5 0 4 which is roughly 50% so let's look at all the possible scenarios in the table here's a table with all the possible scenarios for each door and there are 8 scenario since each door gives us 2 possibilities each and there are 3 doors so we do as before to obtain the probability of each arrangement by multiplying the 3 independent probabilities to get these numbers you can check that these numbers add to 1 and from last video we'll learn that the negative of the logarithm of the probability is the cross entropy so let's go ahead and calculate the cross entropy here we get these numbers and notice that the events with high probability have high cross entropy the events with low probability have a low cross entropy for example the second row which has probability of 0.5 0 4 gives a small cross entropy of point 6 9 and the second-to-last row which is very very unlikely has a probability of 0.3 calculate a formula for the cross entropy here we have our three doors and our sample scenario said that there is a gift behind the first and second doors and no gift behind the third door recall it the probabilities of these events happening are 0.8 for a gift behind the first door open for a gift behind the second door and point 9 for no gift behind the third door so when we calculate the cross entropy we get the negative of the logarithm off the product which is the sum of the negative of the logarithms of the factors which is negative logarithm of 0.8 minus log order to derive the formula we'll add some variables so let's call P 1 the probability that there's a gift behind the first door P 2 the probability there's a gift behind the second door and P 3 to probability there's a gift behind the third door so this point 8 here is P 1 this point 7 here is P 2 and this point 9 here is 1 minus P 3 see it's a probability of there not being a gift is 1 minus the probability of there being a gift and let's have another variable called Y I which will be one of their suppressant behind the ice door and zero if there's no present so why is technically a number of presents behind the ice door in this case y1 equals 1 y2 equals 1 and y 3 equals 0 so we can put all this together and derive a formula for the cross entropy and it's this sum now let's look at the formula inside the summation notice that if there is a present behind the door then why I equals 1 so the first term is logarithm of P I and the second term is 0 likewise if there is no present behind the ice door then Y is 0 so this first term is zero and this term is precisely logarithm of 1 minus P I therefore this formula really encompasses the sums of the negatives of the logarithms which is precisely the cross entropy so the cross entropy really tells us when two vectors are similar or different for example if you calculate the cross entropy of the pair 1 1 0 and point 8 point 7 point 1 we get zero point 6 9 and that is low because 1 arrangement of gifts given by the first set of numbers it's likely to happen based on the probabilities given by the second set of numbers but on the other hand if we calculate the cross entropy of the pair 0 0 1 and point 8 point seven point 1 that is 5 point 12 which is very high this is because the arrangement of gifts being given by the first set of numbers is very unlikely to happen from the probabilities given by the second set of numbers